{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Reviews using Pytorch and TorchText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torch import optim\n",
    "import torch\n",
    "from torch import nn\n",
    "import spacy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Sentiment\n",
       "0  I have bought several of the Vitality canned d...          1\n",
       "1  Product arrived labeled as Jumbo Salted Peanut...          0\n",
       "2  This is a confection that has been around a fe...          1\n",
       "3  If you are looking for the secret ingredient i...          0\n",
       "4  Great taffy at a great price.  There was a wid...          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/jupyter/work/data/Reviews_Sentiment.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    443777\n",
       "0    124677\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the below step if you don't have spacy's model downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 37.4MB 100.4MB/s ta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "  Running setup.py install for en-core-web-sm ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed en-core-web-sm-2.0.0\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /opt/anaconda3/lib/python3.7/site-packages/en_core_web_sm -->\n",
      "    /opt/anaconda3/lib/python3.7/site-packages/spacy/data/en_core_web_sm\n",
      "\n",
      "    You can now load the model via spacy.load('en_core_web_sm')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tokenizer function\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenizer(text): \n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True, batch_first=True)\n",
    "LABEL = data.Field(sequential=False, use_vocab=False, is_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torchtext.data.TabularDataset(path='/home/jupyter/work/data/Reviews_Sentiment.csv', format='csv',\n",
    "                                      fields=[('Text', TEXT), ('Sentiment', LABEL)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568455"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Text', 'Sentiment'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0].__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchtext.data.example.Example"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = train[10]\n",
    "type(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Text', 'Sentiment'])\n",
      "['this', 'is', 'a', 'very', 'healthy', 'dog', 'food', '.', 'good', 'for']\n"
     ]
    }
   ],
   "source": [
    "print(train[10].__dict__.keys())\n",
    "print(train[10].Text[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the below 2 steps if you dont have Glove word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-04-03 14:39:18--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2019-04-03 14:39:18--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  7.65MB/s    in 87s     \n",
      "\n",
      "2019-04-03 14:40:45 (9.49 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /home/jupyter/work/data/glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    }
   ],
   "source": [
    "#!unzip /home/jupyter/work/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([79337, 300])\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train, min_freq=3)\n",
    "TEXT.vocab.load_vectors(torchtext.vocab.Vectors('/home/jupyter/work/data/glove.6B.300d.txt'))\n",
    "print(TEXT.vocab.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.8544e-01,  3.4247e-01,  2.9599e-01, -2.6200e-01,  3.7383e-02,\n",
       "         4.5544e-01,  4.9097e-01,  1.1481e-01, -1.1437e-01, -1.9067e+00,\n",
       "         3.5563e-02, -1.1094e+00, -2.6512e-01,  6.4418e-01, -3.1008e-02,\n",
       "        -3.5130e-01, -1.0547e-03,  7.4658e-02, -3.0369e-01, -2.8188e-01,\n",
       "        -3.4342e-01,  3.6205e-01,  7.1009e-01,  3.0243e-01,  7.0325e-02,\n",
       "         2.9492e-01, -1.6233e-01,  3.0998e-01,  1.3705e-01,  1.1847e-01,\n",
       "        -6.8642e-01,  4.3305e-01, -6.1518e-01,  2.3643e-01, -8.4174e-01,\n",
       "         1.4667e-01, -9.6616e-02, -2.0908e-01, -4.2296e-01, -2.7254e-01,\n",
       "        -7.9343e-01, -6.2781e-01,  6.4804e-01,  1.1541e-01, -3.3486e-01,\n",
       "        -1.4101e-01,  1.2864e-01, -2.5123e-01, -2.6515e-01,  3.0876e-01,\n",
       "        -6.3111e-02,  1.7893e-01,  4.1197e-01,  1.9621e-02, -1.5406e-01,\n",
       "         1.7542e-01,  3.9268e-01,  8.8817e-02,  1.8012e-02, -2.2508e-01,\n",
       "        -3.1832e-01,  2.2296e-02,  5.9453e-01,  5.6538e-02, -7.2464e-01,\n",
       "        -3.1751e-01, -3.8650e-01,  3.3806e-01, -1.6237e-01, -7.6169e-03,\n",
       "         5.2897e-01,  1.4628e-01, -2.2458e-01, -6.6751e-01,  2.3012e-01,\n",
       "        -6.8667e-02,  4.6680e-01,  2.2040e-01, -3.8321e-01, -1.8401e-01,\n",
       "         3.6828e-01,  8.5637e-02, -2.8836e-01,  4.3894e-01,  1.5790e-01,\n",
       "        -1.1443e+00, -1.7327e-01, -2.7353e-03, -3.2621e-01, -2.4110e-01,\n",
       "        -1.4597e-01,  6.1719e-02, -3.0210e-01, -1.6319e-01,  2.5848e-03,\n",
       "         2.6203e-01,  4.5155e-02, -1.9056e-02, -4.8923e-01,  6.2269e-02,\n",
       "         1.2140e-01,  5.4817e-02,  1.1570e-02, -1.6724e-01, -3.0459e-01,\n",
       "        -5.3924e-01, -2.1938e-01,  4.9682e-02, -1.9532e-01,  2.4412e-02,\n",
       "         1.8323e-01, -2.2832e-01, -3.7500e-01, -2.5533e-01,  2.4311e-01,\n",
       "         8.0970e-03,  4.4543e-02,  4.1836e-01,  2.6278e-01,  2.0927e-01,\n",
       "        -4.5486e-01,  1.4258e-01,  3.6823e-01,  6.4177e-01,  4.9307e-02,\n",
       "        -3.8022e-02,  3.3907e-02, -1.1730e-01,  4.0365e-01,  2.3042e-01,\n",
       "         2.0191e-01, -9.9931e-02,  6.0634e-01, -6.9024e-02, -1.1417e-01,\n",
       "        -1.1435e-01,  1.1710e-01,  2.5884e-01, -5.9129e-02,  1.0270e+00,\n",
       "        -6.5444e-02,  3.3082e-01,  4.1195e-01, -5.1409e-01, -4.6449e-01,\n",
       "         4.9719e-01,  1.8368e-01,  1.1482e-01,  2.0175e-01, -4.8580e-01,\n",
       "        -1.1395e-01,  2.4271e-01,  5.3452e-01, -1.7599e-01, -1.8241e-01,\n",
       "         8.5407e-02, -2.7614e-01, -3.3535e-01,  1.3731e-01, -3.8903e-01,\n",
       "         4.7308e-01, -2.8132e-01,  2.9753e-01, -1.6403e-01, -5.1195e-01,\n",
       "         2.5532e-01,  2.6516e-01, -6.1390e-01,  1.3088e-02,  3.5948e-01,\n",
       "         1.6281e-01,  3.1889e-01, -5.7797e-01,  2.5247e-01, -5.5389e-01,\n",
       "         5.6864e-01,  8.9668e-02, -2.0646e-01,  1.0619e-01, -1.2391e-01,\n",
       "        -4.0631e-01, -2.9375e-01, -6.1140e-01, -2.1382e-01,  7.7146e-02,\n",
       "         2.2001e-01,  3.3794e-01,  1.1534e-01,  2.2742e-01, -2.5098e-01,\n",
       "        -1.5659e-01,  7.5970e-01, -1.1835e+00, -3.7791e-01, -3.4801e-01,\n",
       "         2.9379e-01, -4.8425e-01,  1.7500e-01,  1.9507e-01,  8.4789e-01,\n",
       "        -1.8020e-01, -1.4255e-01,  6.1267e-01,  4.8842e-01, -1.8246e-01,\n",
       "         1.5696e-01,  2.9159e-01, -3.3145e-01,  6.5447e-02, -2.6710e-01,\n",
       "        -4.9741e-01, -3.5313e-01, -8.5010e-02,  1.3289e-01, -5.8364e-02,\n",
       "         7.0297e-01,  2.1803e-01, -2.3166e-01,  2.3356e-01,  4.5469e-01,\n",
       "         7.7361e-01, -2.5131e-01, -5.3970e-02, -3.3968e-02, -3.3857e-02,\n",
       "        -2.1060e-01,  2.6651e-01,  4.0503e-01,  2.1139e-01,  3.1596e-01,\n",
       "         2.3324e-01,  4.2179e-01, -4.2393e-01,  2.9181e-01,  1.7072e-01,\n",
       "         4.9736e-01,  3.9993e-01,  7.6354e-02, -5.9329e-01, -3.7345e-02,\n",
       "        -3.9826e-01, -2.8424e-01, -2.6921e-02, -3.1003e-01, -1.4600e+00,\n",
       "         1.2187e-02,  1.9133e-01,  8.5350e-02, -1.6125e-02,  2.6185e-01,\n",
       "         5.2257e-01,  1.1262e-01, -1.4810e-01,  1.5431e-01,  4.8882e-01,\n",
       "         4.1600e-01, -5.3778e-01, -5.1524e-01, -2.7682e-01,  3.1711e-01,\n",
       "        -7.8578e-02, -8.6162e-01, -5.1523e-01, -9.7632e-01,  2.1927e-01,\n",
       "        -8.3264e-04, -2.7921e-03,  2.5754e-01,  2.9383e-01, -2.0229e-01,\n",
       "        -6.1117e-01,  3.1172e-01, -2.1503e-01,  3.8843e-02,  2.6046e-01,\n",
       "        -1.2390e-01, -2.3409e+00,  3.0863e-01, -2.9139e-01, -9.1660e-02,\n",
       "        -2.4405e-01,  1.0426e-01,  2.3199e-01,  2.3967e-01, -1.8697e-01,\n",
       "         1.4248e-01,  7.1032e-01, -3.3381e-01,  1.1111e-01, -1.0267e-01,\n",
       "         2.1123e-01, -1.8594e-01, -7.9114e-02, -1.1809e-01, -8.6420e-02,\n",
       "         1.4090e-01, -2.7209e-01, -4.9944e-01, -2.7510e-01, -6.1361e-01])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.vectors[TEXT.vocab.stoi['food']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "train, val = train.split(split_ratio=0.9, random_state=random.getstate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, pretrained_lm, padding_idx, static=True, hidden_dim=128, lstm_layer=2, dropout=0.2):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_lm)\n",
    "        self.embedding.padding_idx = padding_idx\n",
    "        if static:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding.embedding_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=lstm_layer, \n",
    "                            dropout = dropout,\n",
    "                            bidirectional=True)\n",
    "        self.hidden2label = nn.Linear(hidden_dim*lstm_layer*2, 1)\n",
    "    \n",
    "    def forward(self, sents):\n",
    "        x = self.embedding(sents)\n",
    "        x = torch.transpose(x, dim0=1, dim1=0)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        y = self.hidden2label(self.dropout(torch.cat([c_n[i,:, :] for i in range(c_n.shape[0])], dim=1)))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(epoch, model, eval_every, loss_func, optimizer, train_iter, val_iter, early_stop=1, warmup_epoch=2):\n",
    "    \n",
    "    step = 0\n",
    "    max_loss = 1e5\n",
    "    no_improve_epoch = 0\n",
    "    no_improve_in_previous_epoch = False\n",
    "    fine_tuning = False\n",
    "    train_record = []\n",
    "    val_record = []\n",
    "    losses = []\n",
    "    for e in range(epoch):\n",
    "        train_iter.init_epoch()\n",
    "        for train_batch in iter(train_iter):\n",
    "            step += 1\n",
    "            model.train()\n",
    "            x = train_batch.Text.cuda()\n",
    "            y = train_batch.Sentiment.type(torch.Tensor).cuda()\n",
    "            #print(y.cpu().data.numpy())\n",
    "            model.zero_grad()\n",
    "            pred = model.forward(x).view(-1)\n",
    "            loss = loss_function(pred, y)\n",
    "            losses.append(loss.cpu().data.numpy())\n",
    "            train_record.append(loss.cpu().data.numpy())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if step % eval_every == 0:\n",
    "                model.eval()\n",
    "                model.zero_grad()\n",
    "                val_loss = []\n",
    "                val_accuracy = []\n",
    "                for val_batch in iter(val_iter):\n",
    "                    val_x = val_batch.Text.cuda()\n",
    "                    val_y = val_batch.Sentiment.type(torch.Tensor).cuda()\n",
    "                    val_pred = model.forward(val_x).view(-1)\n",
    "                    #m = nn.Sigmoid()\n",
    "                    #print('val_pred ', val_pred)\n",
    "                    #val_pred_sigmoid = m(val_pred)\n",
    "                    #print('val_pred sigmoid ', val_pred_sigmoid)\n",
    "                    #output = (val_pred_sigmoid > 0.5 ).float()\n",
    "                    #correct = (output == val_y).float().sum()\n",
    "                    val_loss.append(loss_function(val_pred, val_y).cpu().data.numpy())\n",
    "                    #val_accuracy.append(correct.cpu().data.numpy())\n",
    "                val_record.append({'step': step, 'loss': np.mean(val_loss)})\n",
    "                print('epoch {:02} - step {:06} - train_loss {:.4f} - val_loss {:.4f}'.format(\n",
    "                            e, step, np.mean(losses), val_record[-1]['loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_iter = torchtext.data.BucketIterator(dataset=train,\n",
    "                                               batch_size=batch_size,\n",
    "                                               sort_key=lambda x: x.TEXT.__len__(),\n",
    "                                               shuffle=True,\n",
    "                                               sort=False)\n",
    "val_iter = torchtext.data.BucketIterator(dataset=val,\n",
    "                                             batch_size=batch_size,\n",
    "                                             sort_key=lambda x: x.TEXT.__len__(),\n",
    "                                             train=False,\n",
    "                                             sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7994, 889)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_iter), len(val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.data.batch.Batch'>\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter)) # BucketIterator return a batch object\n",
    "print(type(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(batch.Sentiment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 310])\n"
     ]
    }
   ],
   "source": [
    "print(batch.Text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.data.batch.Batch'>\n"
     ]
    }
   ],
   "source": [
    "valid_batch = next(iter(val_iter)) # BucketIterator return a batch object\n",
    "print(type(valid_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(valid_batch.Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 735])\n"
     ]
    }
   ],
   "source": [
    "print(valid_batch.Text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTM(TEXT.vocab.vectors, lstm_layer=1, padding_idx=TEXT.vocab.stoi[TEXT.pad_token], hidden_dim=128).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                    lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 00 - step 001000 - train_loss 0.1709 - val_loss 0.1691\n",
      "epoch 00 - step 002000 - train_loss 0.1686 - val_loss 0.1664\n",
      "epoch 00 - step 003000 - train_loss 0.1678 - val_loss 0.1631\n",
      "epoch 00 - step 004000 - train_loss 0.1662 - val_loss 0.1603\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'Sentiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-f8a71793cefa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m training(model=model, epoch=2, eval_every=1000,\n\u001b[1;32m      2\u001b[0m          \u001b[0mloss_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         val_iter=val_iter)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-fe1648a5a8c7>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(epoch, model, eval_every, loss_func, optimizer, train_iter, val_iter, early_stop, warmup_epoch)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtrain_iter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torchtext/data/batch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, dataset, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, batch, device)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \"\"\"\n\u001b[1;32m    200\u001b[0m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mnumericalize\u001b[0;34m(self, arr, device)\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m                 arr = [numericalization_func(x) if isinstance(x, six.string_types)\n\u001b[0;32m--> 319\u001b[0;31m                        else x for x in arr]\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m                 arr = [numericalization_func(x) if isinstance(x, six.string_types)\n\u001b[0;32m--> 319\u001b[0;31m                        else x for x in arr]\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'Sentiment'"
     ]
    }
   ],
   "source": [
    "training(model=model, epoch=2, eval_every=1000,\n",
    "         loss_func=loss_function, optimizer=optimizer, train_iter=train_iter,\n",
    "        val_iter=val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
